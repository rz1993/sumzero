{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import config\n",
    "import news_handler as nh\n",
    "import scrapers\n",
    "import storage\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5a5028f8a4a4563bdb93f7ac'),\n",
       " 'author': 'https://www.facebook.com/bbcnews',\n",
       " 'description': 'State employees will get $260 (£190) a month after 5% VAT and higher fuel prices came into force.',\n",
       " 'publishedAt': '2018-01-06T01:19:02Z',\n",
       " 'source': {'id': 'bbc-news', 'name': 'BBC News'},\n",
       " 'text': \"Image copyright Reuters Image caption The Saudi government wants to reduce the country's dependence on oil\\n\\nState employees in Saudi Arabia are to be given money to compensate for a new sales tax and a rise in fuel prices.\\n\\nKing Salman has ordered monthly payments of more than $260 (£190) for the next year.\\n\\nThe kingdom has roughly doubled domestic petrol prices and introduced a 5% tax on most goods and services, including food and utility bills.\\n\\nThe Saudi government wants to reduce its dependence on oil following recent turbulence in the crude oil market.\\n\\nThe United Arab Emirates (UAE) has also introduced a 5% sales tax.\\n\\nThe Saudi royal decree says citizens using private health and educations services will be exempt and the state will also pay tax on their first home purchase.\\n\\nIt also says military personnel serving on the front lines with Yemen will also receive a one-off payment of more than $1,000.\\n\\nOrganisations such as the International Monetary Fund have long called for Gulf countries to diversify their sources of income away from oil reserves.\\n\\nIn Saudi Arabia more than 90% of budget revenues come from the oil industry while in the UAE it is roughly 80%.\\n\\nIn 2016 Saudi Arabia announced it was cutting public sector pay to bring down spending, but last year reversed the cuts.\\n\\nAbout two-thirds of working Saudis are employed in the public sector. Their salaries and allowances accounted for 45% of government spending in 2015 and contributed towards a record deficit.\\n\\nSaudi Arabia and the UAE have already taken other steps to boost government coffers.\\n\\nIn Saudi Arabia this included a tax on tobacco and soft drinks as well as a cut in some subsidies offered to locals. In the UAE road tolls have been increased and a tourism tax introduced.\",\n",
       " 'title': 'Saudi Arabia gives handout to compensate for new tax',\n",
       " 'url': 'http://www.bbc.co.uk/news/world-middle-east-42587037',\n",
       " 'urlToImage': 'https://ichef-1.bbci.co.uk/news/1024/cpsprodpb/12D53/production/_91393177_mediaitem91393173.jpg'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_evt = 1\n",
    "dummy_contxt = 2\n",
    "\n",
    "articles = nh.articles_handler(dummy_evt, dummy_contxt, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Connect to the database collection that stores the articles\n",
    "store = storage.Storage(config.DB_URI)\n",
    "store.connect_db(nh.DB_NAME)\n",
    "store.set_collection(nh.ARTICLE_COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load stored articles from document store\n",
    "stored_articles = store.find_all({})\n",
    "\n",
    "# Load the english spacy model and process each article's text\n",
    "nlp = spacy.load('en')\n",
    "docs = [nlp(a.get('text')) for a in stored_articles[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The document is processed through a model, which is a spaCy Pipeline consisting of a tokenizer, tagger, dependency parser and entity recognizer. Aside from the tokenizer, which receives a text string input and outputs a `Doc` object, all other steps in the pipeline apply transformations on a `Doc` object.\n",
    "\n",
    "The `Doc` object exposes an interface of a list of Span objects, which correspond to tokens with a myriad of annotated information attributes, including pointers to other tokens in the document for dependency parses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image NOUN NN compound Image \n",
      "Subtree for 'Image':\n",
      "Image\n",
      "copyright NOUN NN compound Image \n",
      "Subtree for 'copyright':\n",
      "copyright\n",
      "Reuters PROPN NNP compound Image ORG\n",
      "Subtree for 'Reuters':\n",
      "Reuters\n",
      "Image PROPN NNP compound caption \n",
      "Subtree for 'Image':\n",
      "Image copyright Reuters Image\n",
      "caption NOUN NN ROOT caption \n",
      "Subtree for 'caption':\n",
      "Image copyright Reuters Image caption\n",
      "Subjects:  [government, Salman, kingdom, government, Emirates, decree, citizens, state, It, personnel, Organisations, countries, %, it, Arabia, it, salaries, Arabia, this, tax]\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the document span objects and display some of the annotations as well as the subtrees\n",
    "for token in docs[0][:5]:\n",
    "    print(token.text, token.pos_, token.tag_, token.dep_, token.head.text, token.ent_type_)\n",
    "    print(\"Subtree for '{}':\".format(token.text))\n",
    "    print(' '.join([token.text for token in token.subtree]))\n",
    "\n",
    "# We can traverse the dep parse tree by iterating through the tokens once and checking .dep_ and .head for the \n",
    "# dependency relationship we want\n",
    "from spacy.symbols import nsubj, VERB\n",
    "\n",
    "subjects = [t for t in docs[0] if t.dep == nsubj and t.head.pos == VERB]\n",
    "print(\"Subjects: \", subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image copyright Reuters Image caption\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.token.Token' object has no attribute 'root'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-281a548b6292>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoun_chunks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'spacy.tokens.token.Token' object has no attribute 'root'"
     ]
    }
   ],
   "source": [
    "for n in docs[0].noun_chunks:\n",
    "    print(n.text)\n",
    "    print(n[0].)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Matching\n",
    "\n",
    "With `spaCy`, we can also perform rule-based matching which operates on tokens. Matchers except custom patterns described in terms of their token constituents and their attributes. A pattern is a list of dictionaries, with each dictionary being a template for one token in the pattern; a dictionary's key-value pairs act as the criteria for the token's corresponding attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Matcher instance must be initialized with a vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "patterns = [{'POS': 'NOUN'}, {'POS': 'VERB'}, {'POS': 'NOUN'}]\n",
    "predicates = []\n",
    "def match_callback(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i] # start and end correspond to \n",
    "    span = doc[start:end]\n",
    "    span.merge()\n",
    "    predicates.append(span)\n",
    "\n",
    "matcher.add(\"Simple sentence\", match_callback, patterns)\n",
    "matches = matcher(docs[1])\n",
    "\n",
    "print(predicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.bbc.co.uk/news/world-middle-east-42587037'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = articles[0]\n",
    "a.get('url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/home/roland/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-63fb57beb6de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArticle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/roland/anaconda3/lib/python3.5/site-packages/newspaper/article.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_html\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_counter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/roland/anaconda3/lib/python3.5/site-packages/newspaper/article.py\u001b[0m in \u001b[0;36mnlp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mmax_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX_SUMMARY_SENT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0msummary_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/roland/anaconda3/lib/python3.5/site-packages/newspaper/nlp.py\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(url, title, text, max_sents)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0msummaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtitleWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/roland/anaconda3/lib/python3.5/site-packages/newspaper/nlp.py\u001b[0m in \u001b[0;36msplit_sentences\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \"\"\"\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/english.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/roland/anaconda3/lib/python3.5/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/roland/anaconda3/lib/python3.5/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/roland/anaconda3/lib/python3.5/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/home/roland/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "article = Article(a.get('url'))\n",
    "article.build()\n",
    "article.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democrats need to protect 10 Senate Democrats up for re-election in states carried by the president and appeal to the swing voters who could flip control of the House.\n",
      "Yet many of the Senate Democrats considered potential 2020 presidential candidates voted to keep the shutdown going.\n",
      "Those vulnerable Senate Democrats were key to sparking the bipartisan drive to bring the shutdown to an end.\n",
      "The agreement to end the shutdown came after weekend negotiations among a bipartisan group of about two dozen lawmakers assembled by Mr.\n",
      "Republican participation in the bipartisan group was crucial for Democrats seeking assurances from Mr.\n"
     ]
    }
   ],
   "source": [
    "from pyteaser import SummarizeUrl\n",
    "\n",
    "summary = SummarizeUrl(\"https://www.nytimes.com/2018/01/22/us/politics/schumer-democrats-shutdown.html\")\n",
    "for s in summary:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
